# mem mgmt

## toc

- addr binding
- addr spaces
- swapping
- contiguous mem allocations
- fragmentations (external & internal)
- paging
- hardware support for paging
- translation look-aside buffers (tlb)
- mem protection in paged environments
- shared pages
- page sizes
- structure of page tables
  - hashed page tables
  - inverted page tables

### mem is an important resource that must be managed carefully

- mem capacities have been increasing, but programs are getting bigger faster
- parkinson's law
  - programs expand to fill mem available to hold them

### what every programmer would like

- mem with these characteristics
  - private, infinitely large, infinitely fast
  - non-volatile
  - inexpensive
- as of now, there is no such mem

### mem hierarchy

- volatile
  - registers
  - cache
  - main mem
  - electronic disk
- non-volatile
  - magnetic disk
  - optical disk
  - magnetic tapes

## mem mgmt

### why?

- main objective of system is to execute programs
- programs & data must be in mem (at least partially) during execution
- to improve cpu utilization & response times
  - several procs need to be mem resident
  - mem needs to be shared

### mem

- large array of words (bytes)
  - each word has its own addr
- typical program execution cycle
  1. fetch instruction from mem where program is stored
  2. decode
  3. execute. operands may be fetched from mem
  4. result of execution may be stored back to mem

### why procs must be mem resident

- storage that cpu can access directly
  1. registers in cpu
  2. main mem
- machine instructions take mem addrs as arguments
  - none operate on disk addrs
- any instructions in execution plus needed data must be in mem

### overheads in direct access storage devices

- cpus can decode instructions & perform simple operations on register contents - 1+ per clock cycle
- registers accessible in 1 clock cycle
- main mem access is a transaction on mem bus - takes several cycles to complete

### coping with speed differential (hardware)

- introduce fast mem between cpu & main mem for reused data
  - cache (data)
  - cache (instructions)
  - cache (translation)
  - cache (& al.)

### we must also perform correct operation (software)

- os must be protected from accesses by user procs
- user procs must be protected from eachother

### protection: making sure each proc has separate mem spaces

- determine range of legal addrs for proc
- ensure proc can access only those

### providing protection with registers

- base - smallest legal physical addr
- limit - size of range of physical addrs
- e.g. base = 300040, limit = 120900
  - legal: 300040 <--> (300040 + 120900 - 1) = 420939

### base & limit registers loaded only by os

- privileged instructions needed to load registers - executed only in kernel mode
- user programs cannot change contents of these registers
- os given unrestricted access to os & user's mem

### procs & mem

- to execute, a program needs to be placed inside a proc
- proc executes
  - access instructions & data from mem
- proc terminates
  - mem reclaimed & declared available

### binding is a mapping from 1 addr space to the next

- procs can reside in any part of physical mem
  - 1st addr of proc need not be `x0000` (duh)
- addrs in source program are symbolic
- compiler binds symbolic addrs to relocatable addrs
- loader binds relocatable addrs to absolute addrs

### binding can be done at different times

- compile time
  - known that proc will reside at location R
    - if location changes, recompile
  - ms-dos.com programs were bound this way
- load time
  - based on compiler-generated relocatable code
- execution time
  - proc can be moved around during execution
    - binding delayed until runtime
    - special hardware neededd
    - supported by most os

## addr spaces

### addr spaces

- logical
  - addrs generated by program running on cpu
- physical
  - addrs seen by mem unit
- logical addr space - set of logical addrs generated by program
- physical addr space - set of physical addrs corresponding to logical addr space

### mem mgmt unit

- mapping converts logical to physical addrs
- user program never sees real physical addr
  - create pointer to location
  - store in mem, manipulate & compare
- when used as mem addr (load/store)
  - relocated to physical mem

### generation of physical & logical addrs

- compile time & load time
  - identical logical & physical addrs
- execution time
  - logical addrs differ from physical addrs
  - logical addr referred to as virt addr
- runtime mapping performed in hardware via mem mgmt unit (mmu)

#### but do we need to load entire program in mem?

- no. only what we need to execute

### in dynamic loading, an unused routine is never loaded into mem

- routine not loaded until called
  - kept on disk in relocatable load format
- when routine calls another one
  - if routine not present, load it & update addr tables
- does not require special support from os
  - design programs appropriately

### contrasting loading & linking

- loading - load executable into mem prior to execution
- linking - takes some smaller executables & joins them together as a single larger executable

### static linking

- language libs treated as other modules
  - combined by loader into program image
- each program includes a copy of lib functions called in executable image
  - wastes disk/mem space, but makes binary self-contained

### dynamic linking

- similar to dynamic loading
- stub included for each lib reference
  - locate mem resident routine
  - how to load routine if not in mem
- after routine loaded, stub replaces itself with addr of routine
  - subsequent accesses to code segment do not incur dynamic linking costs

### unlike dynamic loading, dynamic linking needs os support

- only os can allow multiple procs to access same mem region
  - shared pages

## swapping

### swapping & mem space restrictions: effects of binding

- proc may/may not be swapped back into same space that it occupied
- binding at compile/load time - difficult to relocate
- execution time binding
  - proc can be swapped into different mem spaces
  - physical addrs computed at runtime

### when cpu scheduler decies to execute a proc, it calls dispatcher

- check whether next proc is in mem
- if next proc not in mem & no free mem
  - swap out a proc that is mem resident
  - swap in desired proc

### overheads in swapping: context switch time

- user proc size: 100MB
- transfer rate: 50MB/s
- transfer time: 2s
- avg latency: 8ms
- swap out = transfer time + latency = 2008ms
- total swap time = swap in + swap out = 4016ms

### factors constraining swapping besides swap time

- proc must be completely idle
  - no pending i/o
- device is busy so i/o is queued
  - swap out P~1~ & swap in P~2~
  - i/o operation may attempt to use P~2~'s mem
    - solution 1: never swap proc with pending i/o
    - solution 2: execute i/o operations into os buffers

### swapping not a reasonable mem mgmt solution

- too much swapping time, too little execution time
- modification of swapping exists in many versions of unix
  - swapping normally disabled
  - starts if many procs are running & set threshold is breached
  - halted when system load reduces

## contiguous mem allocation

- each proc contained in a single continuous setion of mem

### partitioning of mem

- main mem needs to accomodate os & user proc
- divided into 2 partitions
  - resident os (usually low)
  - user procs

### mem mapping & protection

- relocation register - smallest physical addr
- limit register - range of logical addrs
- when cpu scheduler selects a proc for execution
  - relocation & limit registers reloaded as part of context switch
- every addr generated by cpu checked against relocation/limit registers

### mem allocation: fixed partition method

- divide mem into several fixed size partitions
  - each partition contains exactly 1 proc
- degree of multiprogramming bound by num partitions

### mem allocation: variable partition method

- used in batch environments
- os maintains table tracking mem utilization
  - what is available?
  - which ones are occupied?
- initially all mem available
  - considered large mem hole
  - eventually many mem holes will exist
- os orders procs according to scheduling algorithm
- mem allocated to procs until requirements of next proc cannot be met
  - wait until larger block available
  - check if smaller requirements of other procs can be met
- reclaiming spaces
  - when proc arrives if space too large, split into 2
  - when proc terminates
    - if released mem adjacent to other mem holes, fuse to form larger space

### splitting & fusing mem spaces

- scenario: p3 -> split -> p4 -> split -> p1 -> bigger split -> p2

### dynamic storage allocation problem

- satisfying request of size n from set of available spaces
  - 1st fit
  - best fit
  - worst fit

### 1st fit

- scan list of segments until you find a mem gap that is big enough
- gap broken up into 2 pieces
  - 1 for proc
  - other unused

### best fit

- scan entire list beginning -> end
- pick smallest mem gap adequate to host proc

### comparing best & 1st fit

- best fit slower than 1st fit
- surprisingly, best fit also results in more wasted mem than 1st fit
  - tends to fill up mem with tiny, useless gaps

### worst fit

- always take largest available mem gap (perhaps, new gap would be useful)
- simulations have shown worst fit also not good idea

## segmentation

### base & limits translation lacks many features needed to support modern programs

- base & limits translation supports only coarse-grained protection at level of entire proc
  - e.g. not possible to prevent program from overwriting its own code
  - also difficult to share regions of mem between 2 procs
  - since mem for proc needs to be contiguous
    - supporting dynamic mem regions for heaps, thread stacks, or mem-mapped files becomes difficult to impossible

### in our discussions so far

- logical/vram is 1d
  - logical addrs go from 0 to some max val
- many problems can benefit from having 2+ separate logical addr spaces

### a compiler has many tables that are built up as compilation proceeds

- grows continuously as compilation proceeds
  - source text
  - symbol table
    - names & attrs of vars
  - consts table
    - const int, float
  - parse tree
    - syntactic analysis of program
- grows & shrinks in unpredictable ways during compilation
  - stack
    - procedure calls within compiler

### 1d access with growing tables

- program has exceptional num vars
  - in every table, there is an amount of addr space being used, as well as a free section
- hierarchy of addr space
  - symbol table -> source text -> const table -> parse tree -> call stack

### options available to compiler

- say that compilation cannot continue (not cool)
- play robin hood
  - take space from tables with room
  - give to tables with little room

### what would be really cool

- free programmer from having to manage expansion & contraction of tables
- how?
  - provide many completely independent addr spaces (segments)
  - each segment has linear sequence of addrs (0 to max)

### segments & base/limit registers

- hardware supports array of pairs & base & bounds registers for each proc (segment table)
- each entry in array controls a segment of virt addr space
- physical mem for each segment stored contiguously, but different segments can be stored at different locations
  - e.g. code & data segments are not immediately adjacent to each other in either virt or physical addr space

### other things about segments

- different segments can & do have different lengths
- segments grow & shrink independently without affecting each other
  - e.g. consider a segment for stack
    - size increase: sth pushed onto stack segment
    - size decrease: sth popped off stack segment

#### segmentation allows users to view mem as a collection of variable-sized segments

### segmentation

- logical addr space is a collection of segments
- segments have name & length
- addrs specify segment name & offset within segment

### segmentation addring example

- 1400-2400 - segment 0
- 3200-4300 - segment 3
- 4300-4700 - segment 2
- 4800-5800 - segment 4
- 6300-6700 - segment 1

### segmentation hardware

- essentially makes sure that length calculated does not exceed length of physical addr

## fragmentation

### contiguous mem allocation: fragmentation

- as procs & segments are loaded/removed from mem, free mem space broken into small pieces
- external fragmetntation
  - enough space to satisfy request, but available spaces not contiguous

### fragmentation example

- same scenario as before: p3 -> split -> p4 -> split -> p1 -> bigger split -> p2
  - p5 is new segment but cannot be loaded because mem space fragmented

### fragmentation can be internal as well

- mem allocated to proc may be slightly larger than requested
- internal fragmentation - unused mem internal to blocks

### compaction

- solution to external fragmentation
- shuffle mem components
  - objective: place free mem in large block
- not possible if relocation static
  - load time
- approach involves moving procs towards 1 end, gaps towards other

### compaction example

- same scenario as before: p3 -> split -> p4 -> split -> p1 -> bigger split -> p2
  - p5 is new segment but cannot be loaded because mem space fragmented
  - compaction will shift all sections to beginning & allocate more than enough space for p5

### mem compaction time intensive, usually not done

- machine w 1gb ram
- can copy 4 bytes in 20 ns
- time to compact all mem = 10^9 _ (20 _ 10^-9) / 4 = 5s (approx)

## paging: overview of mapping

### overview of how mapping of logical & physical addrs performed

- cpu sends virt addr to mmu
- mmu confers with translation lookaside buffer (tlb)
- mmu may access physical mem to perform translations (pagetable may be stored there)
- mmu sends out physical addr once negotiating finished

### paging mem mgmt scheme

- physical addr space of proc can be non-contiguous
- solves problem of fitting variable-sized mem chunks to backing store
  - backing store has fragmentation problem - compaction impossible

### basic method for implementing paging

- break mem into fixed-size blocks
  - physical mem: frames
  - logical mem: pages
  - blocks are same size
- backing store also divided same way

### cool/odd things about paging

- while a program thinks of its mem as linear, it is usually scattered throughout physical mem
- processor will execute instructions piecemeal using virt addrs
  - virt addrs still linear
  - however, instruction located at end of a page will be located in a completely different region of physical mem from next instruction at start of another page
- data structures appear to be contiguous using virtual addrs, but a large matrix is scattered across many physical page frames

### paging: analogy

- shuffling several decks of cards together
- each proc in its virt addr space sees cards of its own single deck in order
- in physical mem, however, decks of all procs currently running will be shuffled together apparently at random
- page tables are magician's assistant in locating cards from shuffled decks

### paging: logical & physical mem

- logical mem: page 0-3
- page table: { 1, 4, 3, 7 }
- physical mem: { nil, 0, nil, 2, 1, nil, nil, 3 }

### paging: performing addr translation

- cpu reads logical addr (pair of page num & page offset), looks at the page
- cpu finds f & assembles physical addr with page offset, then finds the frame & reads at offset provided

### paging & fragmentation

- no external fragmentation - free frame available for allocation to other procs
- internal fragmentation possible
  - last frame may not be full
  - if proc size independent of page size, internal fragmentation = 1/2 page per proc

### page sizes

- procs, data sets, & mem have all grown over time, page sizes have also increased
- some cpus/kernels support multiple page sizes

### paging: user program views mem as single space

- program scattered throughout physical mem
- user view & physical mem reconciled by addr-translation hardware
- proc has no way of addressing mem outside of its page table

### os manages physical mem

- maintains frame table, 1 entry per frame
  - free or allocated?
  - if allocated, which page of which proc?
- maintains a page table for each proc
  - used by cpu dispatcher to define hardware page table when proc is cpu-bound
    - paging increases context switching time

### e.g. 32-bit addr space

- page size: 4k
- logical addr: `0x23fa427`
  - offset: `0x427`
  - page num: `0x23fa`
- page table entry maps `0x23fa` to frame `0x12345`. what is physical mem addr for logical addr?
  - `0x12345427`
- page size: 1k
- logical addr: `0x23fa427`
  - offset: ~~01~~ 00 0010 0111
  - page num: 0010 0011 1111 1010 01

## hardware support for paging

### purpose of page table is to map virt pages onto physical frames

- page table is like a function
  - takes virt page num as arg, produces physical frame num as result
- virt page field in virt addr replaced by frame field
  - physical mem addr

### 2 major issues facing page tables

- can be extremely large
  - with 4k page size, a 32-bit addr space has 1 million pages
  - also, each proc has its own page table
- mapping must be fast
  - virt-to-physical mapping must be done on every mem reference
  - page table lookup should not be bottleneck

### implementing page table: dedicated registers

- when proc assigned cpu, dispatcher reloads registers
- feasible if page table small, but for most contemporary systems, page table entries are greater than 10^6

### implementing page table in mem

- page table base register (ptbr) points to page table
- 2 mem accesses for each access
  - 1 for page-table entry
  - 1 for byte

### proc & its page table: when page table entirely in memory

- pointer to page table stored in ptbr in pcb similar to program counter
- often, there is also a register which tracks num entries in page table
- page table need not be mem resident when proc swapped out, but must be in mem when proc is running

## translation look-aside buffers (tlb)

### observation

- most programs make large num references to small num pages, not other way around
- only a small fraction of page table entries are heavily read, others barely used at all

### tlb: small, fast-lookup hardware cache

- num tlb entries small (64-1024) - contains few page-table entries
- each entry of tlb consists of 2 parts
  - key & value
- when associative mem presented with item, item compared with all keys simultaneously

### using tlb with page tables

- tlb contains only a few page table entries
- when logical addr generated by cpu, page num presented to tbl
  - when frame num found (tlb hit), use it to access mem
  - usually just 10-20% longer than unmapped mem reference
- tlb miss?
  - mem reference to page table made
  - replacement policies for tlb entries
- some tlbs allow certain entries to be wired down
  - tlb entries for kernel code wired down

### paging hardware with tlb

- tlb essentially acts as middleman on a tlb hit, bypassing page table and directly supplying f

### tlb & addr space identifiers (asids)

- asid uniquely identifies each proc
  - allows tlb to contain addrs from several different procs simultaneously
- when resolving page nums
  - tlb ensures asids match
  - if not, treated as tlb miss

### without asids tlb must be flushed with every context switch

- each proc has its own page table
- without flushing of asids, tlb could include old entries
  - valid virt addrs, but incorrect or invalid physical addrs from previous proc

### effective mem access times

- 20 ns to search tlb
- 100 ns to access mem
- if page in tlb, access time = 20 + 100 = 120 ns
- if page not in tlb, access time = 20 (access tlb) + 100 (access mem to retrieve frame num) + 100 = 220 ns

### effective access times with different hit ratios

- 80% = 0.80 _ 120 + 0.20 _ 220 = 140 ns
- 98% = 0.98 _ 120 + 0.02 _ 220 = 122 ns

### tlb in modern, practical settings

- hit time: 0.5-1 clock cycle
- miss penalty: 10-100 clock cycles
- miss rate: 0.01-1%

## mem protection in paged environments

### protection bits associated with each frame

- kept in page table
- bits can indicate
  - rw, r, x
  - illegal access can be trapped by os
- valid-invalid bit
  - indicates if page in proc's logical addr space

## shared pages

### reentrant code

- computer program or subroutine called reentrant if
  - can be interrupted in middle of execution
  - later can be safely called agan ("re-entered") before previous invocations complete execution
- non-self-modifying
  - does not change during execution
- 2+ procs can
  - execute code at same time
  - will have different data
- each proc has
  - copy of registers & data storage to hold data

### shared pages

- system with n users
  - each user runs text editor
- text editor
  - 150 kb of code
  - 50 kb data space

### shared paging

- other heavily used programs can be shared
  - compilers, runtime libs, database systems, &c.
- to be shareable
  - code must be reentrant
  - os must enforce r nature of shared code

## page sizes

### paging & page sizes

- on average, 1/2 of final page empty
  - internal fragmentation: wasted space
- with n procs in mem, page size p
  - total np/2 bytes of internal fragmentation

### having small pages not necessarily efficient

- small pages mean programs need more pages
  - larger page tables
  - 32kb program needs 4 8kb pages, but 64 512-byte pages
- context switches can be more expensive with small pages
  - need to reload page table

### transfers to & from disk are 1 page at a time

- primary overheads: seek & rotational delays
- transferring small page almost as expensive as transferring page
  - 64 \* 15: 960 ms to load 64 512-byte pages
  - 4 \* 25: 100ms to load 4 8kb pages

### overheads in paging: page table & internal fragmentation

- average proc size: s
- page size: p
- size of each entry: e
- pages per proc: s/p
  - se/p: total page table space
- total overhead: se/p (page table overhead) + p/2(internal fragmentation loss)

### looking at overhead a little closer

- se/p (increases if p small) + p/2 (increases if p large)
- optimum somewhere in between
- 1st derivative with respect to p
  - p^2 = 2se, i.e. p = sqrt(2se)

### optimal page size: considering only page size & internal fragmentation

- p = sqrt(2se)
- s = 128kb, e = 8 bytes per entry
- optimal page size: 1448 bytes
  - in practice, we will never use 1448 bytes
  - instead, either 1k or 2k would be used
    - reason: page sizes are in powers of 2
    - deriving offsets & page nums also easier

### page sizes & size of physical mem

- as physical mem gets bigger, page sizes will get larger too, though not linearly
- quadrupling physical mem size rarely even doubles page size

## structure of page table

### typical use of page table

- proc refers to addrs through pages' virtual addr
- proc has page table
- table has entries for pages that proc uses
  - 1 slot for each page irrespective of validity
- page table sorted by virt addrs

### basic paging hardware

- page table part of hardware

### structure of page table

- hierarchical paging
- hashed page tables
- inverted page tables

### hierarchical paging

- logical addr spaces: 2^32 - 2^64
- page size: 4k = 2^2 \* 2^10 = 2^12
- num page table entries
  - logical addr space size / page size
  - 2^32 / 2^12 = 2^20 ~ 1 million entries
- page table entry = 4 bytes
  - page table for proc = 2^20 \* 4 = 4 mb

### issues with large page tables

- cannot allocate page table contiguously in mem
- solution: divide page table into smaller pieces, page the page-table

### 2-level paging

- for 32-bit logical addrs, there are outer & inner pages of 10, with page offset 12

### addr translation in 2-level paging

- p1 has outer page, p2 has inner page, d is offset
- outer page table traks pages of page table

### computing num of page tables in hierarchical paging

- there is 1 outer table with 2^11 entries
- each outer table entry points to an inner page table so there are 2^11 inner page tables
- total num of page tables = 1 + 2^11
- total num entities = 2^11 + 2^11 \* 2^11

### 2-level paging for 64-bit logical addr space

- outer page has 2^42 entries
- divide outer page table into smaller pieces -> 2nd outer page has 2^32 entries, outer & inner page have 2^10 entries

### why hierarchical tables may strain 64-bit architectures

- in our previous example, there would be 2^32 entries in outer page table
- we could keep going and have 4-level page tables, but all this results in prohibitive num of mem accesses

## hashed page tables

### hashed page tables

- an approach for handling addr spaces > 2^32
- virt page num hashed
  - hash used as key to enter items in hash table
- value part of table is linked list
  - each entry has
    - virt page num
    - value of mapped page frame
    - pointer to next element in list

### searching through hashed table for frame num

- virt page num hashed
  - hashed key has corresponding value in table - linked list of entries
- traverse linked list to find a matching virt page num

### hash tables & 64-bit addr spaces

- each entry refers to several pages instead of a single page
- multiple page-to-frame mappings per entry
  - clustered page tables
- useful for sparse addr spaces where mem references non-contiguous & scattered

## inverted page tables

### inverted page table

- only 1 page table in system that has entry for each mem frame
- each entry tracks
  - proc that owns it (pid)
  - virt addr of page (page num)
- each logical addr has pid, p, and d
- search page table by pid to find the page, then combine with d to get the proper frame

### profiling inverted page table

- decreases amt mem used
- search time increases during page dereferencing
- stored based on frames, but searched on pages
  - whole table might need to be searched!

### other issues with inverted page table

- shared paging
  - multiple pages mapped to same physical mem
- shared paging not possible in inverted tables
  - only 1 virt page entry per physical page
    - stored based on frames

## paging in real-world systems

### x86-64

- intel: ia-64 itanium
  - not much traction
- amd: x86-64
  - intel adopted amd's x86-64 architecture
- 64-bit addr space: 2^64
- currently x86-64 provides
  - 48-bit virt addr (sufficient for 256 tb)
  - page sizes: 4 kb, 2 mb, 1 gb
  - 4-level hierarchical paging

### typical paging scheme in x86-64

- 1st level: 9 bits
- 2nd level: 9 bits
- 3rd level: 9 bits
- 4th level: 9 bits
- offset: 12 bits

### optimization to eliminate levels in x86-64

- high-end servers routinely have 2tb ram
- with 48-bit addressing & 4-level page tables, we can have some optimizations
- each physical frame on x86 is 4 kb
- each page in 4th level page table maps 2 mb
  - if entire 2 mb covered by page table allocated contiguously in physical mem
    - page table entry one layer up can be marked to point directly
- also improves tlb efficiency

### arm architectures

- iphone & android use this
- 32-bit arm
  - 4 kb & 16 kb pages (2-level paging)
  - 1 mb & 16 mb pages (1-level paging)
    - termed sections
  - 2 levels for tlbs: separate tlb for data, another for instructions

## segmentation with paging

### segmentation with paging

- multics: each program can have up to 256k independent segments, each with 64k 36-bit words
- pentium
  - 16k independent segments
  - each segment has 10^9 32-bit words (4 gb)
  - few programs need more than 1000 segments, but many programs need large segments
- 32-bit x86
  - virt addr space within segment has 2-level page table
    - 1st 10 bits top level page table, next 10 bites 2nd level page table, final 12 bits are offset
- 64-bit x86
  - 48 bits of virt addrs within a segment
  - 4-level page table
    - includes optimizations to eliminate 1 or 2 levels of page table
